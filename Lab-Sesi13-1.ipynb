{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/header.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Download Chrome Driver dan Chrome Binary </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# untuk Ubuntu 16.04  \n",
    "# download chrome driver\n",
    "!curl -SL https://chromedriver.storage.googleapis.com/2.37/chromedriver_linux64.zip > chromedriver.zip\n",
    "!unzip chromedriver.zip\n",
    "!rm chromedriver.zip\n",
    "!sudo mv chromedriver /usr/local/bin\n",
    "\n",
    "# download chrome binary\n",
    "!curl https://gist.githubusercontent.com/xxxxlr/18ef864b6374f04939890d9cc186571f/raw/990d54467543d17e5c1874dccc3b7970a63d1ca4/install.sh | bash\n",
    "# !curl https://gist.githubusercontent.com/xxxxlr/18ef864b6374f04939890d9cc186571f/raw/990d54467543d17e5c1874dccc3b7970a63d1ca4/start-chrome.sh | bash\n",
    "\n",
    "#install selenium\n",
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 3632k  100 3632k    0     0  29.8M      0 --:--:-- --:--:-- --:--:-- 29.8M\n",
      "Archive:  chromedriver.zip\n",
      "replace chromedriver? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  9526  100  9526    0     0   387k      0 --:--:-- --:--:-- --:--:--  387k\n",
      "Working in /tmp/google-chrome-installation\n",
      "/tmp/google-chrome-installation /home/ec2-user/SageMaker\n",
      "Configuring the Google Chrome repo in /etc/yum.repos.d/google-chrome.repo\n",
      "Loaded plugins: dkms-build-requires, priorities, update-motd, upgrade-helper\n",
      "amzn-main                                                | 2.1 kB     00:00     \n",
      "amzn-updates                                             | 2.5 kB     00:00     \n",
      "google-chrome                                            | 1.3 kB     00:00     \n",
      "Package wget-1.18-5.30.amzn1.x86_64 already installed and latest version\n",
      "Nothing to do\n",
      "--2019-07-16 16:27:58--  https://dl.google.com/linux/linux_signing_key.pub\n",
      "Resolving dl.google.com (dl.google.com)... 172.217.164.142, 2607:f8b0:4004:815::200e\n",
      "Connecting to dl.google.com (dl.google.com)|172.217.164.142|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 8038 (7.8K) [application/octet-stream]\n",
      "Saving to: ‘linux_signing_key.pub’\n",
      "\n",
      "linux_signing_key.p 100%[===================>]   7.85K  --.-KB/s    in 0s      \n",
      "\n",
      "2019-07-16 16:27:58 (34.3 MB/s) - ‘linux_signing_key.pub’ saved [8038/8038]\n",
      "\n",
      "Attempting a direction installation with yum.\n",
      "Loaded plugins: dkms-build-requires, priorities, update-motd, upgrade-helper\n",
      "Package google-chrome-stable-75.0.3770.142-1.x86_64 already installed and latest version\n",
      "Nothing to do\n",
      "Successfully installed Google Chrome!\n",
      "Requirement already satisfied: selenium in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (3.141.0)\n",
      "Requirement already satisfied: urllib3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from selenium) (1.23)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# untuk redhat, centos\n",
    "# download chrome driver\n",
    "#!curl -SL https://chromedriver.storage.googleapis.com/2.37/chromedriver_linux64.zip > chromedriver.zip\n",
    "#!unzip chromedriver.zip\n",
    "#!rm chromedriver.zip\n",
    "#!sudo mv chromedriver /usr/local/bin\n",
    "\n",
    "# download chrome binary\n",
    "#!curl https://intoli.com/install-google-chrome.sh | bash\n",
    "    \n",
    "#install selenium\n",
    "#!pip install selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Konfigurasi Chrome Driver </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import sys\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Import Library Python </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import Library Python\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Class untuk melakukan query ke Tweeter </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Class untuk mendapatkan tweet dari twitter\n",
    "class TweetsCrawler(object):\n",
    "    #Pause (untuk menunggu loading data)\n",
    "    SCROLL_PAUSE_TIME = 0.3\n",
    "    \n",
    "    def __init__(self):\n",
    "        #Browser (menggunakan chrome driver)\n",
    "        self.browser = webdriver.Chrome('chromedriver',options=chrome_options)\n",
    "        #Base URL untuk mencari tweet\n",
    "        self.base_url = 'https://twitter.com/search?q='\n",
    "        self.proceeding_url = \"\"\n",
    "    def get_data(self, query, scroll=0):\n",
    "        ''' \n",
    "        Fungsi untuk mencari tweet.\n",
    "        \n",
    "        Parameter Doc.\n",
    "          @Param: query\n",
    "          @Desc: string query yang ingin dicari\n",
    "          \n",
    "          @Param: scroll\n",
    "          @Default: 0\n",
    "          @Desc: berapa banyak kali scroll yang diinginkan. (mensimulasikan scroll layaknya pada web browser)\n",
    "        '''\n",
    "        \n",
    "        #Buka URL\n",
    "        self.browser.get(self.base_url+query+self.proceeding_url)\n",
    "\n",
    "        #dapatkan html tag <body>\n",
    "        body = self.browser.find_element_by_tag_name('body')\n",
    "        \n",
    "        #Simulasikan scroll\n",
    "        for _ in range(scroll):\n",
    "            #Sleep untuk menunggu loading data\n",
    "            time.sleep(TweetsCrawler.SCROLL_PAUSE_TIME)\n",
    "            #Scroll ke bawah\n",
    "            self.browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "        #Daftar tweet\n",
    "        all_tweets = []\n",
    "        \n",
    "        #Parsing HTML dengan bs4\n",
    "        html = BeautifulSoup(body.get_attribute('innerHTML'), 'html.parser')\n",
    "        #Select result\n",
    "        result = html.select('#timeline li.stream-item')\n",
    "        #Untuk semua Tweet dalam result\n",
    "        for tweet in result:\n",
    "            #Ambil tweet id\n",
    "            tweet_id = tweet['data-item-id']\n",
    "            #Ambil tweet text\n",
    "            tweet_text = tweet.select('p.tweet-text')[0].get_text()\n",
    "            #masukkan ke daftar tweet\n",
    "            all_tweets.append({\"id\": tweet_id, \"text\": tweet_text})\n",
    "            \n",
    "        #Kembalikan tweet sebagai Pandas DataFrame\n",
    "        return pd.DataFrame(all_tweets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Class untuk HashTag Crawler </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "class HashTagCrawler(TweetsCrawler):\n",
    "    def __init__(self):\n",
    "        TweetsCrawler.__init__(self)\n",
    "        self.base_url = 'https://twitter.com/hashtag/'\n",
    "    def get_data(self, query, scroll=0):\n",
    "        data = TweetsCrawler.get_data(self, query, scroll)\n",
    "        hashtags = data.text.str.findall(r'#.*?(?=\\s|$)')\n",
    "        return pd.DataFrame(set(flatten(hashtags)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Query Tweet di Twitter </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     id                                               text\n",
      "0   1150877677315903488  Peter Theil thinks @Google $googl should be in...\n",
      "1   1149722914922618880  2019 top digital transformation tech investmen...\n",
      "2   1150696313438294016  How #Blockchain Will Improve Your Big Data\\n#B...\n",
      "3   1151027286679810048  The future of farming will rely on big data to...\n",
      "4   1151129889929605120  [EBOOK]  Smart Analytics: The smart way to pro...\n",
      "5   1150990958814793728  How can you find value in 4 steps with Big Dat...\n",
      "6   1148935019143950336  What is the #InternetOfThings? Why is it so im...\n",
      "7   1151104689422757888  Germany’s ‘BIG DATA’ technology is largely dri...\n",
      "8   1151050236284067840  How to track objects on the factory floor or s...\n",
      "9   1148481165784825856  My Big Data professor mentioned Amitab Bachcha...\n",
      "10  1147787598976487424  Searching for #BigData Capital of the World ht...\n",
      "11  1151139381677215745  Hottest Big Data Analytics Trends that will do...\n",
      "12  1150079999220822019  The 3 most-scarce tech skills include big data...\n",
      "13  1150996864612872192  #DATA JOBS Sunnyvale United States - Senior Bi...\n",
      "14  1151059930629181440  Explore Hadoop Big Data Analytics market worth...\n",
      "15  1150809762709327873  We are super excited to give you a sneak peek ...\n",
      "16  1150665719786266624  The Ministry of Agriculture and Farmers Welfar...\n",
      "17  1150560039003987971  Urgency for Big Data Analytics Infographic #in...\n",
      "18  1150465287713677313  What are the 3 distinct #IoT tiers in which #a...\n",
      "19  1150374590318829569  Big Data, AI & IoT Part Two: Driving Industry ...\n",
      "20  1150776028203298817  Panel: one big challenge for legal analytics i...\n",
      "21  1150647281324109824  NSR’s Big Data via Satellite Analytics Analysi...\n",
      "22  1150149667130396673  Why Prescriptive Analytics Is the Future of Bi...\n",
      "23  1150142519583346688  #Data is the Fuel for #AI: Data Collection Met...\n",
      "24  1149926937541992448  #Cybele_H2020 will facilitate better use of ex...\n",
      "25  1149709092447039489  Cloud-based #Vertica Analytics Platform underp...\n",
      "26  1149669272110731264  Size Matters! Corporations are growing bigger ...\n",
      "27  1149574762345062402  What is the #InternetOfThings? Why is it so im...\n",
      "28  1149384171636363265  How can you save time building #bigdataanalyti...\n",
      "29  1149377908173103107  How can research funders tap into big data and...\n",
      "30  1149222952786788352  #cloudcomputing via @NodeXL http://bit.ly/2GYZ...\n",
      "31  1149172456827117568  Taming Big Data with Spark Streaming and Scala...\n",
      "32  1148980703754891266  How A Deluge Of Data Is Changing The IT Landsc...\n",
      "33  1148655277937516545  Pictured is @jdhooms presenting to the VP of o...\n",
      "34  1148609133547020289  @AlianzaCAOBA participando en Workshop Big Dat...\n",
      "35  1148606496936120326  Web and social media analytics: A data-driven ...\n",
      "36  1148589251531026432  An alternative start on the day without data a...\n",
      "37  1148230273336827906  Listen in on a high level discussion of #bigda...\n",
      "38  1148117107965669376  Mirror the future#Innovation\\n\\nBuild compelli...\n"
     ]
    }
   ],
   "source": [
    "#Buat object Selenium Client\n",
    "tc = TweetsCrawler()\n",
    "\n",
    "#Ambil Tweets dengan query \"Big Data Analytics\"\n",
    "tweets_df = tc.get_data('Big Data Analytics', 10)\n",
    "\n",
    "#Tampilkan Hasil\n",
    "print(tweets_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Crawling HashTag </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    0\n",
      "0                                               #IoT?\n",
      "1                                       #CyberAttacks\n",
      "2                                                #iot\n",
      "3                                     #DataScientists\n",
      "4               #Healthcarepic.twitter.com/QkdBZw1p1N\n",
      "5             #Infographicspic.twitter.com/enRVpCkfge\n",
      "6                                      #surveillance,\n",
      "7                                            #Machine\n",
      "8                                      #SatelliteData\n",
      "9                                          #PokemonGO\n",
      "10                                    #cloudcomputing\n",
      "11                             #Digitaltransformation\n",
      "12                                     #DataAnalytics\n",
      "13   #SocialPrescribing2019pic.twitter.com/6IM1NbzjUn\n",
      "14                  #hadooppic.twitter.com/mSGKRGb8Pn\n",
      "15                #Mobilitypic.twitter.com/1umcyjCwK2\n",
      "16                                  #DataMonetization\n",
      "17                                      #FutureofWork\n",
      "18                               #MachineIntelligence\n",
      "19                                      #DataLiteracy\n",
      "20                                           #infosec\n",
      "21              #Algorithmspic.twitter.com/divWqWQ7Ds\n",
      "22                                         #Marketing\n",
      "23                                                #iA\n",
      "24                                #AutonomousVehicles\n",
      "25              #Statisticspic.twitter.com/WbAcgZag0a\n",
      "26                              #socialmediamarketing\n",
      "27                                        #blockchain\n",
      "28                                          #privacy,\n",
      "29                                       #SmartCities\n",
      "..                                                ...\n",
      "253                                          #Hacking\n",
      "254                                          #Digital\n",
      "255         #sustainabilitypic.twitter.com/g2dqvfYE4l\n",
      "256                                           #London\n",
      "257                                        #marketing\n",
      "258                                           #RecSys\n",
      "259                                      #bikeability\n",
      "260                                #FacialRecognition\n",
      "261                                      #KobmaxQueen\n",
      "262                                          #Science\n",
      "263                                        #SpaceData\n",
      "264                                             #anal\n",
      "265                                         #blogging\n",
      "266                                     #DeepMedicine\n",
      "267                                 #digitalmarketing\n",
      "268                                 #MondayMotivation\n",
      "269                                               #BI\n",
      "270                                               #AI\n",
      "271                                      #influencers\n",
      "272  #ReinforcementLearningpic.twitter.com/IskvmGYCW1\n",
      "273                                       #Blockchain\n",
      "274                                        #ecommerce\n",
      "275                              #influencermarketing\n",
      "276                                   #dataextraction\n",
      "277                                       #Statistics\n",
      "278                                         #Robotics\n",
      "279                                              #Iot\n",
      "280                                               #RT\n",
      "281                                      #datascience\n",
      "282                                     #WearableTech\n",
      "\n",
      "[283 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "#Buat object Selenium Client\n",
    "hc = HashTagCrawler()\n",
    "\n",
    "#Ambil Tweets dengan query \"Big Data Analytics\"\n",
    "hashtag_df = hc.get_data('BigData', 10)\n",
    "\n",
    "#Tampilkan Hasil\n",
    "print(hashtag_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"img/thumbs-up.png\"></img>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
